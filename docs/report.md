# Darwin's Playground

## Definicija problema

V sistemu bodo dve osnovni vrsti bitij:

- Å¾rtve (rastlinojedci),
- plenilci (mesojedci).

Plenilci potrebujejo tako vodo (morajo najti vodni vir) kot hrano (Å¾rtve), medtem ko Å¾rtve potrebujejo hrano
(vir hrane, kot so rastline) in vodo (vir z vodo). Fokus te raziskovalne naloge bodo Å¾rtve in njihove strategije
za preÅ¾ivetje v danem okolju. Plenilci imajo bolj omejen dostop do virov, kar pomeni, da bodo vedno na preÅ¾i za
rastlinojedci.

Agenti globalnega znanja o okolju nimajo. Znanje si gradijo na podlagi raziskovanja okolja in informacij, ki
jih dobijo od ostalih agentov.

Strategije agentov bodo zasnovane na podlagi Q-uÄenja, kjer pa bo poudarek predvsem na izmenjavi informacij
med agenti iste vrste. Izmenjava informacij bo pripomogla k boljÅ¡emu uspehu populacije kot celote kot tudi
agenta samega.

### Okolje

Okolje bo predstavljeno s pomoÄjo dvodimenzionalne ploÅ¡Äe, kjer bodo posamezni kvadratki predstavljali
razliÄne dele povrÅ¡ja (voda, pesek, viri hrane itd.). Agenti bodo za premikanje po razliÄnih ploÅ¡Äah
porabili razliÄno koliÄino energije. Treba je poudariti tudi, da imajo agenti samo lokalno znanje, tj.
poznajo samo polja, ki so jih Å¾e obiskali, ostala pa so jim nevidna.

Okolje je dinamiÄno in se lahko spreminja skozi Äas. Lokacija, koliÄina in uporabnost virov (voda, hrana)
se lahko spreminjajo zaradi razliÄnih zunanjih dejavnikov, kot so vremenske spremembe (npr. deÅ¾, suÅ¡a, mrak)
ali sezonske spremembe, ki vplivajo na koliÄino hrane in vode v okolju. Te spremembe bodo vplivale na strategije
preÅ¾ivetja agentov, ki se bodo morali hitro prilagoditi novim pogojem.

### ObnaÅ¡anje agentov

Agenti bodo lahko v okolju izvedli veÄ razliÄnih akcij:

- gibanje: navzgor, navzdol, levo, desno,
- prehranjevanje: v primeru, da se nahajajo ob viru hrane,
- komunikacija: izmenjava informacij z ostalimi agenti.

Cilj je, da se razvijejo optimizirane strategije za preÅ¾ivetje v tem dinamiÄnem okolju, ob upoÅ¡tevanju
spreminjajoÄih se virov in pogojev.

### Lastnosti agentov

Agenti imajo razliÄne lastnosti, ki vplivajo na njihovo obnaÅ¡anje in strategijo preÅ¾ivetja:

- energija (potrebna za vse akcije, ko je doseÅ¾ena energija niÄ, agent zaÄne izgubljati Å¾ivljenje),
- Å¾ivljenjske toÄke (ko se agent poÅ¡koduje, le-te izgubi, ko je doseÅ¾ena vrednost niÄ, agent umre).

Poleg tega vsak agent hrani koristne informacije o okolju, ki jih uporabi kot del strategije za premikanje
po okolju. To so informacije, kot so:

- lokacija virov,
- zadnja videna lokacija plenilcev ...

### Primeri iz realnega sveta

Problemi iz realnega sveta, ki so primerljivi s to nalogo so:

- strateÅ¡ke video igre kot npr. *Sid Meierâ€™s CivilizationÂ® VI*, kjer se uporablja kompleksno ravnanje z viri,
  prilagajanje na dinamiÄno okolje in interakcija med razliÄnimi agenti (npr. enotami, naravnimi viri, naselji),
- bioloÅ¡ki ekosistemi, kjer se spremljajo interakcije med plenilci in Å¾rtvami, prilagajanje na sezonske spremembe
  in evolucijske strategije za preÅ¾ivetje,
- nadzorovanje naravnih virov v simulacijah, kot so sistemi za upravljanje z vodo, energijo ali gozdovi, kjer
  razliÄni agenti (npr. podjetja, skupnosti, drÅ¾ave) sprejemajo odloÄitve, ki vplivajo na okolje in druge akterje,
- robotika in avtonomni sistemi, kjer roboti sodelujejo in tekmujejo za vire v dinamiÄnem okolju, kot so pri reÅ¡evanju
  nalog v nesreÄah ali za iskanje virov na teÅ¾ko dostopnih obmoÄjih ...

### KljuÄne besede

1. VeÄagentni sistemi,
2. Simulacija prilagajanja,
3. Optimizacija strategij,
4. Q-Learning,
5. UÄenje s krepitvijo.

## Evaluacija problema

SledeÄa dela so bila v pomoÄ pri naÄrtovanju evaluacije reÅ¡itve:

1. [Holland, J. H. (1975). "Adaptation in Natural and Artificial Systems"](http://repo.darmajaya.ac.id/3794/1/Adaptation%20in%20Natural%20and%20Artificial%20Systems_%20An%20Introductory%20Analysis%20with%20Applications%20to%20Biology%2C%20Control%2C%20and%20Artificial%20Intelligence%20%28A%20Bradford%20Book%29%20%28%20PDFDrive%20%29.pdf),
2. [Wooldridge, M. (2002). "An Introduction to MultiAgent Systems"](https://uranos.ch/research/references/Wooldridge_2001/TLTK.pdf),
3. [Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for Multi-Robot Cooperation Tasks](https://arxiv.org/html/2407.08164v2),
4. [An Introduction to Centralized Training for Decentralized Execution in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2409.03052),
5. [Collaborative multi-agent reinforcement learning based on experience propagation](https://ieeexplore.ieee.org/document/6587341),
6. [A Distributed Q-Learning Algorithm for Multi-Agent Team Coordination](https://ieeexplore.ieee.org/document/1526928),
7. [Ray](https://docs.ray.io/en/latest/rllib/index.html).

Nekatere izmed metod, ki so bile opisane v Älankih so sledeÄe:

- HC-MARL (Hierarchical Consensus-based Multi-Agent Reinforcement Learning),
- CTDE (Centralized Training Decentralized Execution),

Glavna prednost teh metod v primerjavi z osnovnimi metodami uÄenja s krepitvijo je kooperacija razliÄnih agentov
za doseg cilja. Agenti lahko na podlagi lokalnega znanja in izmenjave informacij pridejo do konsenza in
sprejemajo boljÅ¡e odloÄitve, ki bodo vodile v bolj uspeÅ¡no raziskovanje okolja.

V veÄagentskem ojaÄitvenem uÄenju je kljuÄen izziv prilagajanje dejanj posameznega agenta dinamiÄnemu okolju
izboljÅ¡anje celotne uÄinkovitosti sistema. VeÄina algoritmov predpostavlja, da agent pozna strukturo igre ali
Nashovo ravnoteÅ¾je ter da ima informacije o dejanjih in nagradah drugih agentov. Ker nagrade temeljijo tako na
lastnih kot na dejanjih sodelujoÄih agentov, lahko MDP model ojaÄitvenega uÄenja obravnavamo kot veÄagentsko
Markovljevo igro [5].

Pri uÄenju agenti uporabljajo strategije raziskovanja, kot sta Îµ-greedy in Boltzmanova eksploracija, saj na
zaÄetku nimajo zadostnega znanja o okolju. Zaradi omejenih izkuÅ¡enj pogosto veÄkrat obiÅ¡Äejo ista stanja, kar
postane problematiÄno v veÄjih prostorskih stanjih. KljuÄen izziv pri sodelovalnih veÄagentskih sistemih je
omogoÄanje izmenjave znanja med agenti. Predlagana optimizacijska metoda cikliÄnih poti omogoÄa izvleÄek
optimalnih poti iz izkuÅ¡enj, ki jih lahko agenti ali skupine delijo za hitrejÅ¡o konvergenco vrednostne funkcije [5].

### Kolaborativno veÄ-agentno uÄenje s krepitvijo na osnovi propagacije izkuÅ¡enj

Opisana optimizacijska metoda cikliÄnih poti omogoÄa izvleÄek optimalnih poti iz izkuÅ¡enj, ki jih lahko agenti
ali skupine delijo za hitrejÅ¡o konvergenco vrednostne funkcije [5].

### CTDE

Metoda CTDE je sploÅ¡na metoda, kjer uÄenje agentov poteka centralno â€“ delitev znanja. Agenti pa akcije izvedejo
neodvisno drug od drugega na podlagi svojih preteklih izkuÅ¡enj. Agenti lahko v fazi uÄenja uporabijo informacije,
ki jim med decentraliziranim izvajanjem niso bile na voljo.

Nekateri tovrstni pristopi uporabljajo deljeno Q-funkcijo v Äasu uÄenja. Izguba (loss) se tako za vsakega agenta
v Äasu uÄenja izraÄuna kot skupna Q-funkcija. PoslediÄno imajo agenti tudi tekom decentraliziranega izvajanja
nekaj znanja od ostalih agentov [4].

### DQN in DRQN

DQN (Deep Q-network) je nadgradnja Q-uÄenja, ki za funkcijo aproksimacije uporablja nevronsko mreÅ¾o. Vendar
pa ima DQN informacije o celotnem okolje, zato ta metoda ni relevantna za najin problem.

DRQN pa reÅ¡i problem pomanjkljivih podatkov o okolju, saj omogoÄa delo z delnimi podatki. DRQN temelji na RNN
modelu, katerega namen pa je, da hrani zgodovino vhodnih vrednosti in jo ponovno uporabi v prihodnjih
napovedih [4].

### QMIX

QMIX je metoda za veÄagentsko ojaÄitveno uÄenje, ki razÅ¡irja VDN (Value Decomposition Networks) tako, da
omogoÄa bolj sploÅ¡ne razgradnje vrednostnih funkcij. Namesto preproste vsote lokalnih Q-funkcij, QMIX modelira
skupno Q-funkcijo kot monotono nelinearno funkcijo posameznih Q-funkcij agentov. To pomeni, da lahko agenti Å¡e
vedno izbirajo akcije na podlagi svojih lokalnih Q-vrednosti, kar omogoÄa uÄinkovitejÅ¡e usposabljanje in
decentralizirano izvajanje.

QMIX uporablja mixing network, ki zagotavlja monotono kombinacijo posameznih Q-funkcij s pomoÄjo hiperomreÅ¾ij,
ki generirajo uteÅ¾i. Med uÄenjem se omreÅ¾je trenira end-to-end, pri Äemer se optimizira napaka med napovedano
skupno Q-funkcijo in ciljno vrednostjo, ki temelji na diskontiranih prihodnjih nagradah. Metoda se dobro obnese
v okoljih, kjer agenti lahko delujejo preteÅ¾no neodvisno, vendar ima omejitve pri nalogah, kjer je nujno
usklajevanje med agenti [4].

### HC-MARL

Metoda HC-MARL uporablja veÄ slojev konsenza: konsenz na dolgi rok in konsenz na kratki rok. Kratka opazovanja
v okolju sproÅ¾ijo takojÅ¡en, nizko-nivojski konsenz, medtem ko daljÅ¡a opazovanja okolja sproÅ¾ijo bolj strateÅ¡ki,
visoko nivojski konsenz. Algoritem pa uporablja tudi mehanizem za prilagajanje pomembnosti vsakega nivoja, ki se
spreminja skupaj z dinamiÄnim okoljem. Na ta naÄin agenti bolj strateÅ¡ko sprejemajo odloÄitve, ki prinaÅ¡ajo
takojÅ¡njo nagrado in odloÄitve, ki nagrado prinaÅ¡ajo na dolgi rok. HC-MARL je posebej zanimiv za najin primer,
saj omogoÄa agentom, da kombinirajo kratkoroÄne in dolgoroÄne strategije preÅ¾ivetja v dinamiÄnem okolju [3].

## NaÄrt reÅ¡itve

### Informacije o skupini

Izbrana projektna skupina je skupina Å¡t. 4.

Na tem projektu pa delava sledeÄa Å¡tudenta:

- David Jerman,
- AndraÅ¾ Å kof.

### TehniÄne informacije o projektu

Projekt je dostopen na spletni strani GitHub in sicer na sledeÄem naslovu:
[Darwin's Playground](https://github.com/DavidJerman/Darwins-Playground).

Poglaviten programski jezik izbran za izvedbo tega projekta je `Python`, ki pa se bo kombiniral
z raznimi programskimi knjiÅ¾nicami, ki bodo omogoÄile laÅ¾ji razvoj konÄne reÅ¡itve. Glavna knjiÅ¾nica
za implementacijo tega projekta bo `Ray` [7].

### Opis reÅ¡itve

Najprej, kljuÄno je definirati okolje: kje se nahaja hrana (npr. doloÄene toÄke s hrano) in voda (morda doloÄena
obmoÄja ali toÄke). Agenti (Å¾ivali) se bodo gibali po tem okolju, pri Äemer bodo s pomoÄjo Q-learninga poskuÅ¡ali
nauÄiti optimalne poti do hrane in vode.

Postopek implementacije bi lahko sledil tem korakom:

Stanja (States): DoloÄimo stanja, ki predstavljajo razliÄne lokacije v okolju. Vsako stanje bi lahko predstavljalo
toÄko na zemljevidu, kjer je mogoÄe najti hrano ali vodo.

Akcije (Actions): Vsaka Å¾ival lahko izvaja doloÄene akcije, kot so premiki na sosednje toÄke v okolju.

Nagrade (Rewards): Definiramo nagrade, ki jih agent prejme, ko pride do hrane ali vode. Na primer, pozitivna nagrada
za dosego hrane ali vode, negativna za izgubo energije zaradi neuspeÅ¡nega iskanja.

Q-table: Zgradimo Q-table, ki hrani vrednosti Q(stanje, akcija) za vsako kombinacijo stanja in akcije. Ta tabela se
posodablja glede na izkuÅ¡nje agentov (Å¾ivali) v okolju.

UÄenje (Learning): Agenti se premikajo po okolju, izvajajo akcije in posodabljajo vrednosti v Q-tabli na podlagi
prejetih nagrad in izbranih akcij.

Raziskovanje in izkoriÅ¡Äanje: Pomembno je najti ravnovesje med raziskovanjem novih poti (razliÄne akcije) in
izkoriÅ¡Äanjem nauÄenih informacij (npr. optimalne poti do hrane in vode).

Nadgradnja Q-learninga, s tem, da se omogoÄi komunikacijo med agenti. Agent, ki Å¾e pozna del mape lahko ta del ob
komunikaciji opiÅ¡e drugemu agentu, ki se s tem to nauÄi. Nadgradnja bi temeljila na pristopu CTDE opisanem
zgoraj.

### Evaluacija reÅ¡itve

UspeÅ¡nost algoritma ocenjujemo z veÄ kljuÄnimi metrikami, ki merijo, kako dobro se agenti prilagajajo okolju ter
kako hitro in uÄinkovito se uÄijo optimalnih strategij preÅ¾ivetja.

1. Hitrost konvergence: Merimo, koliko epizod je potrebno, da agenti doseÅ¾ejo stabilno strategijo gibanja do hrane in
   vode. HitrejÅ¡a konvergenca pomeni bolj uÄinkovit uÄni proces.
2. PovpreÄna pridobljena nagrada: Spremljamo povpreÄno nagrado agentov skozi iteracije. ViÅ¡ja povpreÄna nagrada pomeni,
   da agenti bolje optimizirajo svoje poti in zmanjÅ¡ujejo nepotrebno porabo energije.
3. DeleÅ¾ uspeÅ¡nih epizod: IzraÄunamo, kolikÅ¡en deleÅ¾ simulacij agenti uspeÅ¡no zakljuÄijo s pridobitvijo hrane in vode.
   ViÅ¡ji deleÅ¾ uspeÅ¡nih epizod nakazuje boljÅ¡e prilagajanje algoritma.
4. DolÅ¾ina poti do cilja: Merimo povpreÄno Å¡tevilo korakov, ki jih agent potrebuje za dosego hrane ali vode. KrajÅ¡e
   poti pomenijo bolj optimalne strategije gibanja.
5. Stopnja deljenja informacij: Ker agenti lahko med seboj delijo znanje, ocenjujemo, kako hitro in uÄinkovito se novo
   pridobljene informacije prenaÅ¡ajo med agenti in kako to vpliva na uÄenje celotnega sistema.
6. Robustnost algoritma: Preverjamo, kako dobro agenti prilagajajo svoje strategije v spreminjajoÄem se okolju, na
   primer ob spreminjanju razpoloÅ¾ljivosti virov ali dodajanju novih agentov.

Na podlagi teh metrik lahko primerjamo razliÄne strategije in ocenimo, ali izboljÅ¡ave, kot je deljenje informacij med
agenti, dejansko prispevajo k hitrejÅ¡emu in boljÅ¡emu uÄenju.

### Iterativen razvoj projekta

#### Iteracija 1: Osnovna struktura

1. Osnovna implementacija agentov in okolja ter njihovih lastnosti na
   podlagi [RLlibâ€™s MultiAgentEnv API](https://docs.ray.io/en/latest/rllib/multi-agent-envs.html#rllib-multi-agent-environments-doc),
2. Osnovna delovanje agentov â€“ gibanje, iskanje virov.

#### Iteracija 2: PoveÄanje kompleksnosti agentov in okolja

1. Implementacija Q-uÄenja za agente,
2. Osnovne strategije preÅ¾ivetja.

#### Iteracija 3: Nadgradnja Q-uÄenja

1. Deljenje znanja med agenti,
2. Kooperativne strategije (nadgradnja Q-uÄenja).

#### Iteracija 4: Povratne informacije, izboljÅ¡ave in dokumentacija

1. Optimizacija performance,
2. KonÄno testiranje,
3. Evaluacija razliÄnih pristopov.

![Diagram](diagram.svg)

### PoroÄilo napredka

Delo v posameznih sprintih je razdeljeno glede na Älane ekipe.

#### Sprint 1

David Jerman:

- vzpostavitev delovnega okolja (RLlib + torch) in prilagoditev za sisteme z grafiÄno kartico Nvidia,
- vzpostavitev osnovnega okolja na
  podlagi [RLlibâ€™s MultiAgentEnv API](https://docs.ray.io/en/latest/rllib/multi-agent-envs.html#rllib-multi-agent-environments-doc),
- zagon primerov knjiÅ¾nice RLlib,
- dodan diagram okolja.

Okolje ni bilo v celoti implementirano, saj sem naletel na teÅ¾ave z okoljem RLlib, in sicer je
bilo potrebno dodatno namestiti vse potrebne CUDA/CUD-nn knjiÅ¾nice ter pravilno verzijo torch, ki
podpira zagon Q-learning-a na grafiÄni kartici. Prav tako je bilo potrebno zaÄetno okolje
prilagoditi, da je kompatibilno s knjiÅ¾nico RLlib. Nenazadnje pa sem se moral spoznati tudi s
samo knjiÅ¾nico, katere prej Å¡e nisem uporabljal.

#### Sprint 2

David Jerman:

Kaj je bilo narejeno:

- manÅ¡i popravki,
- analiza primerov PPO (algoritem na osnovi nevronskih mreÅ¾ za veÄ-agentni sistem - vsebovan v RLLib),
- implementacija osnovnega okolja za PPO,
- uÄenje, klasifikacija in vizualizacija s PPO za dano okolje.

Kje sem naletel na teÅ¾ave:

Ko sem Å¾elel PPO uporabiti v kombinaciji z najinim okoljem, sem naletel na veÄ problemov:

- manjkajoÄi algoritmi v RLLib - QDN sploh ni prisoten v najnovejÅ¡i razliÄici,
- teÅ¾ave z delovanjem na Windows sistemu,
- nazadnje sem reÅ¡itev prestavil v WSL2, namestil druge verzije knjiÅ¾njic in primeri so zaÄeli delovati,
- naletel pa sem tudi na problem, kjer v primeru, da uporabim bolj kompleksna opazovanja pri agentih (
  torej da poleg lokacij opazujem Å¡e druge okoljske lastnosti), algoritem ponovno odpove. Za reÅ¡evanje
  tega problema sem ravno tako zapravil precej Äasa, a je dokumentacija na spletu slaba, napake, ki pa jih
  dobim pa ne povejo niÄ uporabnega.

Cilj za naslednji teden:

- usposobiti celotno okolje za PPO,
- nadgraditi okolje,
- zaÄeti z delom na najinem algoritmu (CTDE Q-Learning).

#### Sprint 3

Repozitorij: [Darwin's Playground](https://github.com/DavidJerman/Darwins-Playground)

David Jerman:

Ta teden sem dokonÄal vse kar sem si zadal za ta Å¡print:

1. Popravil sem agentovo opazovanje okolja. Zaradi pomanjkljive dokumentacije sem imel kar precej dela.
   Agent zdaj lahko prejme razliÄne podatke, kot so njegova pozicija, pozicija hrane, vrsta terena itd.
   Glavna reÅ¡itev je bila uporaba konektorja, ki vhodne podatke zravna (flatten). Dodal sem tudi en sloj po meri.
2. Odstranil sem stare datoteke in obstojeÄo kodo dokaj enostavno integriral v trenutno ogrodje, saj je bila
   Å¾e prej narejena v skladu z RLLib.
3. IzboljÅ¡al sem kompatibilnost s strojno in programsko opremo na razliÄnih napravah. Algoritem zdaj uporabi CUDA,
   Äe je na voljo, sicer pa preklopi na CPU. Uporabnik lahko z zastavico --device izbere Å¾eleno napravo.
4. Dodal sem nekaj privzetih vrednosti za laÅ¾ji zagon.
5. Na koncu sem dodal Å¡e moÅ¾nost uporabe razliÄnih algoritmov (poleg PPO, ki ga uporabljava). Ti algoritmi Å¡e niso
   v celoti podprti, saj imajo doloÄene specifike, ki jih bo treba Å¡e implementirati.

AndraÅ¾ Å kof:

Ustvarili sem mreÅ¾o ploÅ¡Äic (ang. tiles), kjer vsaka ploÅ¡Äica predstavlja doloÄen tip terena. V implementaciji
sem vkljuÄil tri osnovne tipe terena:

Trava (ğŸŒ¿)

Pesek (ğŸœï¸)

Skale (â›°ï¸)

Postopek generacije:
Inicializacija terena:
Na zaÄetku je celotna mreÅ¾a zapolnjena z nakljuÄno doloÄenim peskom in skalami, nato pa se dodajo gruÄe trave.

Povezane travnate povrÅ¡ine:
Trava ni veÄ razporejena povsem nakljuÄno â€“ pazili smo, da se pojavlja v povezanih skupinah po 5â€“10 ploÅ¡Äic, da
simulira bolj realistiÄno naravno okolje (npr. travnike).

Vizualizacija:
KonÄni rezultat je bil izpisan v ukazni vrstici, kjer vsak tip terena predstavlja doloÄen emoji, kar omogoÄa hiter
vizualni pregled nad razporeditvijo terena.
